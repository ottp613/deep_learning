{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197fbb2d",
   "metadata": {},
   "source": [
    "# Assignment 2 – Zero-Shot Image Classification with Transformers\n",
    "\n",
    "In this assignment, you will apply a pre-trained vision–language transformer (e.g. CLIP) to perform **zero-shot** classification on the Fashion-MNIST dataset—classifying each image without any task-specific training. You will build on the concepts from Assignment 1 by comparing this “off-the-shelf” approach to the CNN you previously trained.\n",
    "\n",
    "You will:\n",
    "1. **Load** the Fashion-MNIST images using PyTorch instead of Keras.\n",
    "2. **Run a zero-shot baseline** with simple text prompts to set a performance reference.\n",
    "3. **Engineer improved prompts** and measure the resulting accuracy gains.\n",
    "4. **Visualise image embeddings** with UMAP to inspect class separability.\n",
    "5. **Conduct one mini-experiment** of your choice.\n",
    "6. **Summarise findings** and reflect on strengths and weaknesses of zero-shot transformers versus a trained CNN.\n",
    "\n",
    "# 1. Loading the Fashion-MNIST Dataset\n",
    "\n",
    "As in assignment 1, we'll load the Fashion-MNIST dataset, but this time using `torchvision.datasets` to ensure compatibility with the `transformers` library. We will also load our model and processor from the `transformers` library.\n",
    "\n",
    "The transformers library allows us to use pre-trained models like CLIP, which can perform zero-shot classification by leveraging the text prompts we provide. There are two key objects we will use: the `CLIPModel` for the model itself and the `CLIPProcessor` for preparing our images and text prompts.\n",
    "\n",
    "Since we are not actually training a model in this assignment, we will set the CLIP model to evaluation mode. If the model is designed to utilize features like dropout or batch normalization, setting it to evaluation mode ensures that these features behave correctly during inference (prediction). Setting the model to evaluaton mode also tells PyTorch that we don't have to compute gradients, which can save memory and speed up inference.\n",
    "\n",
    "In order to speed up processing, we will also move the model to an \"accelerator\" if available. This is typically a GPU, but modern MacBooks also have an \"Apple Silicon\" accelerator that can be used for inference, called MPS (Metal Performance Shaders). If you are using a MacBook with Apple Silicon, you can use the MPS device for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026463ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if required\n",
    "# pip install transformers torchvision torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4701b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e68f0929cc34cb9a7a8e42abd21c2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PAVAN\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4719a946c04355bab64961c99294dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcd562236584567b6a22f85170a64d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041df3e0328146df807f03fef6c28674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa22c070c254f1981251841c0c1aa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19af6dad8d304af99b5db553d74b5d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afbfc97aaef48f1b9508c2f9ac2e05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b2f0f6592c43e0883f5358a10bfb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a53153e4494585be0c5e6f37250b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d2bb364c33465299c2f67bc46caf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model     = CLIPModel.from_pretrained(clip_model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name, use_fast=False)\n",
    "\n",
    "# Set model to evaluation mode, as we are not training it\n",
    "clip_model.eval()\n",
    "\n",
    "# Check for accelerators\n",
    "device = \"cpu\" # Default to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # Use GPU if available\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "clip_model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14789e25",
   "metadata": {},
   "source": [
    "Now we are ready to load the testing set from Fashion-MNIST. We will use the `torchvision.datasets.FashionMNIST` class to load the dataset. We do not need to apply any transformations to the images, as the `CLIPProcessor` ensures any input images are in the format that the model is trained on.\n",
    "\n",
    "You should:\n",
    "\n",
    "- [ ] Use the `torchvision.datasets.FashionMNIST` class to load the *test* split of the dataset. Documentation is available [here](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html).\n",
    "- [ ] Create a PyTorch `DataLoader` to iterate over the dataset in batches. Use a batch size of 16 and set `shuffle=True` to randomise the order of the images. You will also need to supply the provided `collate_clip` function to the `DataLoader collate_fn` argument to ensure the images are processed correctly. Documentation for `DataLoader` is available [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2da3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:19<00:00, 1.33MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 192kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:04<00:00, 890kB/s] \n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\"\n",
    "]\n",
    "\n",
    "def collate_clip(batch):\n",
    "    imgs, labels = zip(*batch) # Unzip the batch into images and labels\n",
    "    proc = clip_processor(images=list(imgs),\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True) # Process images with CLIPProcessor\n",
    "    # Send pixel_values to GPU/CPU now; labels stay on CPU for metrics\n",
    "    return proc[\"pixel_values\"].to(device), torch.tensor(labels)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\",train=False,download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_clip)\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed01b1c",
   "metadata": {},
   "source": [
    "If your code is correct, the following cell should show the first batch of images from the Fashion-MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d97e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the first batch of images from `test_loader`\n",
    "\n",
    "def show_batch(loader):\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images.cpu()  # Move images to CPU for plotting\n",
    "    # Renormalize to [0, 1] for visualization\n",
    "    images = (images - images.min()) / (images.max() - images.min())\n",
    "    _, axes = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for ax, img, label in zip(axes, images, labels):\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.set_title(CLASS_NAMES[label.item()])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_batch(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1183726",
   "metadata": {},
   "source": [
    "We’re now ready to run our zero-shot classification baseline!\n",
    "\n",
    "# Brief Introduction to Zero-Shot Classification\n",
    "\n",
    "In Assignment 1, we followed the typical machine-learning pipeline: we trained a CNN on the Fashion-MNIST dataset, using labelled examples to update the model’s weights. While effective, that approach requires a curated, task-specific training set—a luxury you don’t always have in practice.\n",
    "\n",
    "Zero-shot classification flips the script.  A large vision–language model (VLM) such as **CLIP** is first pre-trained on hundreds of millions of image–text pairs scraped from the web.  Because it learns *joint* visual–textual embeddings, the model can later solve new tasks simply by “measuring” how similar an image is to a **text prompt** that describes each candidate class—without seeing a single task-labelled example.\n",
    "\n",
    "**How it works**  \n",
    "1. Feed an image through CLIP’s vision encoder → **image feature**.  \n",
    "2. Feed a textual prompt (e.g. “a photo of a sandal”) through CLIP’s text encoder → **text feature**.  \n",
    "3. Compute cosine similarity between the image feature and every class’s text feature.  \n",
    "4. Pick the class whose prompt is most similar.\n",
    "\n",
    "For our first attempt, we’ll use the bare class names as prompts, e.g.:\n",
    "\n",
    "- \"T-shirt/top\"\n",
    "- \"Trouser\"\n",
    "\n",
    "### You should:\n",
    "\n",
    "- [ ] Build embeddings: use the `get_text_embeddings` helper function to create text embeddings for the class names.\n",
    "- [ ] Run inference: use the `get_image_embeddings` helper function to create image embeddings.\n",
    "- [ ] Compute cosine similarity: complete and use the `get_cosine_similarity` helper function to compute the cosine similarity between the image and text embeddings.\n",
    "- [ ] Make predictions: use the `get_predictions` helper function to get the predicted class for each image in the batch.\n",
    "\n",
    "Note that for normalized vectors like the ones we are using, cosine similarity is equivalent to the dot product. This means we can use the handy formula `cosine_similarity = vector_a @ vector_b.T` to compute the similarity between the image and text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c3acc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(class_names: list[str]) -> torch.Tensor:\n",
    "    \"\"\"    Get text embeddings for the given class names using CLIP.\n",
    "    Args:\n",
    "        class_names (list[str]): List of class names to encode.\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized text embeddings for the class names.\n",
    "    \"\"\"\n",
    "    tokenized = clip_processor(text=class_names,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_text_features(**tokenized)\n",
    "    \n",
    "    # Check if 'outputs' is the object or the tensor\n",
    "    if hasattr(outputs, 'pooler_output'):\n",
    "        text_embeddings = outputs.pooler_output\n",
    "    elif hasattr(outputs, 'text_embeds'):\n",
    "        text_embeddings = outputs.text_embeds\n",
    "    else:\n",
    "        # If it's already a tensor, this will just pass it through\n",
    "        text_embeddings = outputs\n",
    "\n",
    "    text_feats = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "    return text_feats\n",
    "\n",
    "def get_image_embeddings(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"    Get image embeddings for the given images using CLIP.\n",
    "    Args:\n",
    "        images (torch.Tensor): Batch of images to encode.\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image embeddings for the images.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_image_features(pixel_values=images.to(device))\n",
    "    \n",
    "    # Access the tensor attribute\n",
    "    if hasattr(outputs, 'image_embeds'):\n",
    "        image_embeddings = outputs.image_embeds\n",
    "    elif hasattr(outputs, 'pooler_output'):\n",
    "        image_embeddings = outputs.pooler_output\n",
    "    else:\n",
    "        image_embeddings = outputs\n",
    "\n",
    "    image_feats = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return image_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16653654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_cosine_similarity(image_feats: torch.Tensor, text_feats: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image features and text features.\n",
    "    Args:\n",
    "        image_feats (torch.Tensor): Image features of shape (N, D).\n",
    "        text_feats (torch.Tensor): Text features of shape (M, D).\n",
    "    Returns:\n",
    "        numpy.ndarray: Cosine similarity matrix of shape (N, M), where N is the number of images and M is the number of text prompts.\n",
    "    \"\"\"\n",
    "    image_feats = image_feats.cpu()  # Ensure image features are on CPU\n",
    "    text_feats = text_feats.cpu()    # Ensure text features are on CPU\n",
    "\n",
    "    # Compute cosine similarity, which is the dot product of normalized vectors\n",
    "    return (image_feats @ text_feats.T).numpy()  # Convert to NumPy array\n",
    "\n",
    "def get_predictions(similarity: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get predictions based on cosine similarity scores.\n",
    "    Args:\n",
    "        similarity (numpy.ndarray): Cosine similarity matrix of shape (N, M), where N is the number of images and M is the number of text prompts.\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted class indices for each image, shape (N,).\n",
    "    \"\"\"\n",
    "    # Get the index of the maximum similarity for each image\n",
    "    return np.argmax(similarity, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e1de",
   "metadata": {},
   "source": [
    "With these functions complete, you are ready to run the zero-shot classification baseline. Complete the code to follow these steps:\n",
    "\n",
    "- [ ] Build text embeddings for the class names using the `get_text_embeddings` function (this only needs to be done once).\n",
    "- [ ] For each batch of images:\n",
    "    - [ ] Get image embeddings using the `get_image_embeddings` function.\n",
    "    - [ ] Compute cosine similarity between the image and text embeddings using the `get_cosine_similarity` function.\n",
    "    - [ ] Save the predictions so that we can build a confusion matrix later.\n",
    "- [ ] Report the accuracy of the predictions and the confusion matrix using the `accuracy_score` and `confusion_matrix` functions from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4364acd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m text_feats = get_text_embeddings(CLASS_NAMES)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pixel_values, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     image_feats = \u001b[43mget_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     similarity = get_cosine_similarity(image_feats, text_feats)\n\u001b[32m      9\u001b[39m     predictions = get_predictions(similarity)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mget_image_embeddings\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"    Get image embeddings for the given images using CLIP.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    images (torch.Tensor): Batch of images to encode.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m    torch.Tensor: Normalized image embeddings for the images.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     outputs = \u001b[43mclip_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Access the tensor attribute\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[33m'\u001b[39m\u001b[33mimage_embeds\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\utils\\generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:853\u001b[39m, in \u001b[36mCLIPModel.get_image_features\u001b[39m\u001b[34m(self, pixel_values, interpolate_pos_encoding, **kwargs)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    827\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_image_features\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    833\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m | BaseModelOutputWithPooling:\n\u001b[32m    834\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[33;03m    Examples:\u001b[39;00m\n\u001b[32m    836\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    851\u001b[39m \u001b[33;03m    ...     image_features = model.get_image_features(**inputs)\u001b[39;00m\n\u001b[32m    852\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     vision_outputs: BaseModelOutputWithPooling = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m     pooled_output = vision_outputs.pooler_output\n\u001b[32m    860\u001b[39m     vision_outputs.pooler_output = \u001b[38;5;28mself\u001b[39m.visual_projection(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:675\u001b[39m, in \u001b[36mCLIPVisionTransformer.forward\u001b[39m\u001b[34m(self, pixel_values, interpolate_pos_encoding, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n\u001b[32m    673\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.pre_layrnorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m last_hidden_state = encoder_outputs.last_hidden_state\n\u001b[32m    681\u001b[39m pooled_output = last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:507\u001b[39m, in \u001b[36mCLIPEncoder.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m hidden_states = inputs_embeds\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m encoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     hidden_states = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(\n\u001b[32m    514\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    515\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:389\u001b[39m, in \u001b[36mCLIPEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m residual = hidden_states\n\u001b[32m    388\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:358\u001b[39m, in \u001b[36mCLIPMLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    356\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.fc1(hidden_states)\n\u001b[32m    357\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "CLASS_NAMES = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "text_feats = get_text_embeddings(CLASS_NAMES)\n",
    "for pixel_values, labels in test_loader:\n",
    "    image_feats = get_image_embeddings(pixel_values)\n",
    "    similarity = get_cosine_similarity(image_feats, text_feats)\n",
    "    predictions = get_predictions(similarity)\n",
    "    y_true.extend(labels.cpu().numpy())\n",
    "    y_pred.extend(predictions)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Report the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857f4a8",
   "metadata": {},
   "source": [
    "Reflection: Consider the results. How does the performance of this zero-shot baseline compare to the CNN you trained in Assignment 1? What are the strengths and weaknesses of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e16af",
   "metadata": {},
   "source": [
    "## Improving Zero-Shot Classification with Prompt Engineering\n",
    "\n",
    "In the previous section, we directly used the class names as text prompts for zero-shot classification. However, we can often improve performance by crafting more descriptive prompts that better capture the visual characteristics of each class. For example, instead of just \"T-shirt/top\", we could use \"a photo of a T-shirt\" or \"a photo of a top\". This additional context can help the model make more accurate predictions.\n",
    "\n",
    "In this section, we will experiment with more detailed prompts for each class to see if we can improve the zero-shot classification performance. You should:\n",
    "\n",
    "- [ ] Create a list of improved prompts for each class. For example, instead of just \"T-shirt/top\", you could use \"a photo of a T-shirt\" or \"a photo of a top\".\n",
    "- [ ] Use the `get_text_embeddings` function to create text embeddings for the improved prompts.\n",
    "- [ ] Run the zero-shot classification baseline again using the improved prompts and report the accuracy and confusion matrix.\n",
    "\n",
    "Note: Take advantage of the confusion matrix above. If two classes are often confused, consider how you might improve the prompts to help the model distinguish them better.\n",
    "\n",
    "The aim for this section is for you to improve the performance of the model. However, if you find that the performance does not improve significantly, you can still reflect on the process and consider how you might further refine the prompts with more effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5abf5cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m y_true, y_pred = [], []\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pixel_values, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     img_features = \u001b[43mget_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     sim_matrix = (img_features.cpu() @ text_features.cpu().T).numpy()\n\u001b[32m     51\u001b[39m     preds = np.argmax(sim_matrix, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mget_image_embeddings\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m     27\u001b[39m images = images.to(device)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     outputs = \u001b[43mclip_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[33m'\u001b[39m\u001b[33mimage_embeds\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     33\u001b[39m     image_embeddings = outputs.image_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\utils\\generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:853\u001b[39m, in \u001b[36mCLIPModel.get_image_features\u001b[39m\u001b[34m(self, pixel_values, interpolate_pos_encoding, **kwargs)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    827\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_image_features\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    833\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m | BaseModelOutputWithPooling:\n\u001b[32m    834\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[33;03m    Examples:\u001b[39;00m\n\u001b[32m    836\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    851\u001b[39m \u001b[33;03m    ...     image_features = model.get_image_features(**inputs)\u001b[39;00m\n\u001b[32m    852\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     vision_outputs: BaseModelOutputWithPooling = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m     pooled_output = vision_outputs.pooler_output\n\u001b[32m    860\u001b[39m     vision_outputs.pooler_output = \u001b[38;5;28mself\u001b[39m.visual_projection(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:675\u001b[39m, in \u001b[36mCLIPVisionTransformer.forward\u001b[39m\u001b[34m(self, pixel_values, interpolate_pos_encoding, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n\u001b[32m    673\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.pre_layrnorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m last_hidden_state = encoder_outputs.last_hidden_state\n\u001b[32m    681\u001b[39m pooled_output = last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:507\u001b[39m, in \u001b[36mCLIPEncoder.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m hidden_states = inputs_embeds\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m encoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     hidden_states = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(\n\u001b[32m    514\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    515\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:389\u001b[39m, in \u001b[36mCLIPEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m residual = hidden_states\n\u001b[32m    388\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:358\u001b[39m, in \u001b[36mCLIPMLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    356\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.fc1(hidden_states)\n\u001b[32m    357\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PAVAN\\Documents\\gitrepos\\dl-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "improved_prompts = [f\"a photo of a {name}\" for name in class_names]\n",
    "\n",
    "\n",
    "def get_text_embeddings(prompts: list[str]) -> torch.Tensor:\n",
    "    tokenized = clip_processor(text=prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_text_features(**tokenized)\n",
    "    \n",
    "    if hasattr(outputs, 'text_embeds'):\n",
    "        text_embeddings = outputs.text_embeds\n",
    "    elif hasattr(outputs, 'pooler_output'):\n",
    "        text_embeddings = outputs.pooler_output\n",
    "    else:\n",
    "        text_embeddings = outputs \n",
    "    \n",
    "    return text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def get_image_embeddings(images: torch.Tensor) -> torch.Tensor:\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_image_features(pixel_values=images)\n",
    "    \n",
    "\n",
    "    if hasattr(outputs, 'image_embeds'):\n",
    "        image_embeddings = outputs.image_embeds\n",
    "    elif hasattr(outputs, 'pooler_output'):\n",
    "        image_embeddings = outputs.pooler_output\n",
    "    else:\n",
    "        image_embeddings = outputs\n",
    "\n",
    "    \n",
    "    return image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_features = get_text_embeddings(improved_prompts)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for pixel_values, labels in test_loader:\n",
    "    img_features = get_image_embeddings(pixel_values)\n",
    "    \n",
    "    sim_matrix = (img_features.cpu() @ text_features.cpu().T).numpy()\n",
    "    \n",
    "    preds = np.argmax(sim_matrix, axis=1)\n",
    "    \n",
    "    y_true.extend(labels.tolist())\n",
    "    y_pred.extend(preds.tolist())\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Zero-shot Accuracy with Improved Prompts: {accuracy * 100:.2f}%\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix: Improved Prompts')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e88284",
   "metadata": {},
   "source": [
    "Reflection: How did your detailed prompts affect the zero-shot classification performance? Did you see a significant improvement compared to the baseline? What insights did you gain about the model's understanding of the classes? Do you think that with more effort you could further improve the performance? If so, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817d7b4",
   "metadata": {},
   "source": [
    "## Visualizing Image Embeddings with UMAP\n",
    "\n",
    "To better understand how the model perceives the different classes, we can visualize the image embeddings using UMAP (Uniform Manifold Approximation and Projection). UMAP is a dimensionality reduction technique that helps us see how similar or dissimilar the embeddings are in a lower-dimensional space.\n",
    "\n",
    "By visualizing the embeddings, we can gain insights into how well the model can distinguish certain images, even without considering the text prompts. This can help us identify clusters of similar images and see if there are any overlaps between classes.\n",
    "\n",
    "You should:\n",
    "\n",
    "- [ ] Use the `get_image_embeddings` function to get the image embeddings for the entire test set.\n",
    "- [ ] Use UMAP to reduce the dimensionality of the image embeddings to 2D.\n",
    "- [ ] Plot the 2D embeddings, coloring each point by its true class label.\n",
    "\n",
    "You may need to install the `umap-learn` library if you haven't already. You can do this by running `pip install umap-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a20757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install UMAP if you haven't already\n",
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Collect image embeddings\n",
    "# ------------------------------------------------------------\n",
    "all_img_emb = []\n",
    "all_labels  = []\n",
    "\n",
    "for pixel_values, labels in test_loader:\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Fit UMAP\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Plot coloured by ground-truth label\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f943e",
   "metadata": {},
   "source": [
    "The UMAP embeddings allow us to see how separable or non-separable different classes are with our specific model. If two specific images are very similar, then they will be placed near each other on this graph. \n",
    "\n",
    "Reflection: Do you notice any challenges in distinguishing images based on this figure? Are there any types of clothing in the dataset which the model has no trouble distinguishing from the others?\n",
    "\n",
    "## Mini-Experiment\n",
    "\n",
    "In this section, you will conduct a mini-experiment of your choice to further explore the capabilities of zero-shot classification with transformers. This can be anything you'd like, but here are some ideas to get you started.\n",
    "\n",
    "### A. Alternative Model\n",
    "\n",
    "So far we have been utilizing OpenAI's CLIP model for zero-shot classification. However, there are many other vision–language models available in the `transformers` library that you can experiment with. For example, there are larger CLIP models such as [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14), and open-source versions such as [laion/CLIP-ViT-B-32-laion2B-s34B-b79K](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K). You can also search huggingface [here](https://huggingface.co/models?sort=trending&search=clip) to find other models that might be suitable for zero-shot classification.\n",
    "\n",
    "You can try using a different model to see if it improves the zero-shot classification performance. You should:\n",
    "- [ ] Load a different model and processor from the `transformers` library.\n",
    "- [ ] Run the zero-shot classification baseline with the new model and report the accuracy and confusion matrix.\n",
    "- [ ] Reflect on the performance of the new model compared to the original CLIP model\n",
    "  - How does the new model perform compared to the original CLIP model?\n",
    "  - Do you notice any differences in the types of errors made by the new model?\n",
    "\n",
    "### B. Multiple-Description Classification\n",
    "\n",
    "Another interesting experiment is to explore multiple-description classification. *This involves providing multiple text prompts for each class, allowing the model to choose the most relevant one. For example, instead of just \"T-shirt/top\", you could provide \"a photo of a T-shirt\", \"a photo of a top\", and \"a photo of a shirt\". This can help the model better understand the class and increases the likelihood of a correct prediction. You should:\n",
    "\n",
    "- [ ] Create a list of multiple prompts for each class.\n",
    "- [ ] Use the `get_text_embeddings` function to create text embeddings for the multiple prompts.\n",
    "- [ ] Run the zero-shot classification baseline again using the multiple prompts and report the accuracy and confusion matrix.\n",
    "- [ ] Consider the model to be correct if it guesses *any* of the prompts belonging to the correct class.\n",
    "\n",
    "### C. Top-K Classification\n",
    "\n",
    "In some classification tasks, it can be useful to consider if the right answer is among the top K (e.g. top 3) predictions. This can be particularly useful in cases where the model is uncertain or when there are multiple similar classes. You should:\n",
    "\n",
    "- [ ] Modify the `get_predictions` function to return the top K predictions for each image.\n",
    "- [ ] Modify the accuracy calculation to consider the model correct if the true class is among the top K predictions.\n",
    "- [ ] Report the accuracy and confusion matrix for the top K predictions. Report at least two different values of K (e.g. K=2 and K=4).\n",
    "\n",
    "### D. Other Ideas\n",
    "\n",
    "You are welcome to come up with your own mini-experiment! Explain your idea in the report and implement it. Did it work as you expected? What did you learn from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af85f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "234eab38",
   "metadata": {},
   "source": [
    "### Short Report\n",
    "\n",
    "In this section, you will write a short report summarizing your findings from the mini-experiment. The report should include the following sections:\n",
    "\n",
    "- **Introduction**: Briefly describe the mini-experiment you conducted and its objectives.\n",
    "- **Methodology**: Explain the steps you took to conduct the experiment, including any modifications you made to the code or model.\n",
    "- **Results**: Present the results of your experiment.\n",
    "- **Discussion**: Reflect on the performance of the model and the implications of your findings. Consider the strengths and weaknesses of zero-shot transformers versus a trained CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745659f3",
   "metadata": {},
   "source": [
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `23:59 PM - 02/11/2025`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb)\n",
    "    * The Lab 4 notebook (labs/lab_4.ipynb)\n",
    "    * The Lab 5 notebook (labs/lab_5.ipynb)\n",
    "    * The Lab 6 notebook (labs/lab_6.ipynb)\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/deep_learning/pull/<pr_id>`\n",
    "* Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-7-help-ml`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
